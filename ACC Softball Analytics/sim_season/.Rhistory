n = 1000
p = 300
beta = runif( n=p, min=1, max=5) * (-1)^(runif(p) < .5)
X = cbind( 1, matrix( rnorm(n*(p-1)), ncol=p-1))
py = 1 / ( 1 + exp(-X %*% beta))
y = c(runif(n) < py)
df = function(b){
sigmoid = 1 / (1 + exp(-c(X %*% b)))
return( colSums((sigmoid - y) * X) )
}
neg_l = function(b){
sigmoid = 1 / (1 + exp(-c(X %*% b)) +10^-6) # 10^-6 for numerical stability
return( -sum(y * log(sigmoid) + (1 - y) * log(1 - sigmoid)) )
}
b0 = runif(p) * 2*sqrt(6/p) + (-sqrt(6/p))
beta_MLE = gradient_descent( gradient=df, f=neg_l, x0=b0)[1000,]
py_hat = 1 / (1 + exp(-c(X %*% beta_MLE)))
y_hat = (py_hat > .5) # Using a threshold of .5
# False positive rate
FPR = sum(y_hat[y == 0] == 1) / sum(y == 0); FPR
# True postive rate (i.e., sensitivity)
TPR = sum(y_hat[y == 1] == 1) / sum(y == 1); TPR
# True negative rate (i.e., specificity)
TNR = sum(y_hat[y == 0] == 0) / sum(y == 0); TNR
# How do these values change for different thresholds?
grid = seq( 0, 1, by=.001)
FPR = rep( NA, length(grid))
TPR = rep( NA, length(grid))
for(k in 1:length(grid)){
y_hat = (py_hat > grid[k])
FPR[k] = sum(y_hat[y == 0] == 1) / sum(y == 0)
TPR[k] = sum(y_hat[y == 1] == 1) / sum(y == 1)
}
# Plot the ROC curve
plot( FPR, TPR, lwd=2, type="l")
lines( grid, grid, lty="dotted", lwd=2)
# -----------------------------------------------------------------------------
n = 2000
p = 1000
stochastic_gradient_descent = function( r, n, gradient, eta=.01, x0, epsilon=10^-3, max_iter=10^4){
t = 2
trace = matrix( NA, ncol=length(x0), nrow=max_iter)
trace[1:2,] =	rbind( x0, x0 + 2*epsilon)
# Begin the gradient descent algorithm
while( sum((trace[t,] - trace[t-1,])^2)^.5 >= epsilon & t < max_iter ){
index = sample( 1:n, size=r, replace=F)
# Update the parameters
trace[t+1,] = trace[t,] - eta * gradient( trace[t,], index)
#print(trace[t+1,])
t = t +1
}
cat("final learning rate = ", eta, "\n")
if(t == max_iter){
print("Warning: max iterations exceeded")
trace[max_iter,] = NA
}
return(trace[1:t,])
}
stochastic_gradient_descent = function( r, n, gradient, eta=.01, x0, epsilon=10^-3, max_iter=10^4){
t = 2
trace = matrix( NA, ncol=length(x0), nrow=max_iter)
trace[1:2,] =	rbind( x0, x0 + 2*epsilon)
# Begin the gradient descent algorithm
while( sum((trace[t,] - trace[t-1,])^2)^.5 >= epsilon & t < max_iter ){
index = sample( 1:n, size=r, replace=F)
# Update the parameters
trace[t+1,] = trace[t,] - eta * gradient( trace[t,], index)
#print(trace[t+1,])
t = t +1
}
cat("final learning rate = ", eta, "\n")
if(t == max_iter){
print("Warning: max iterations exceeded")
trace[max_iter,] = NA
}
return(trace[1:t,])
}
n = 2000
p = 1000
beta = runif( n=p, min=1, max=5) * (-1)^(runif(p) < .5)
X = cbind( 1, matrix( rnorm(n*(p-1)), ncol=p-1))
py = 1 / ( 1 + exp(-X %*% beta))
y = c(runif(n) < py)
# Train the model
df = function(b){
sigmoid = 1 / (1 + exp(-c(X %*% b)))
return( colSums((sigmoid - y) * X) )
}
neg_l = function(b){
sigmoid = 1 / (1 + exp(-c(X %*% b)) +10^-6) # 10^-6 for numerical stability
return( -sum(y * log(sigmoid) + (1 - y) * log(1 - sigmoid)) )
}
b0 = runif(p) * 2*sqrt(6/p) + (-sqrt(6/p))
beta_MLE = c(tail( stochastic_gradient_descent( gradient=df, f=neg_l, x0=b0), 1))
n = 2000
p = 1000
beta = runif( n=p, min=1, max=5) * (-1)^(runif(p) < .5)
X = cbind( 1, matrix( rnorm(n*(p-1)), ncol=p-1))
py = 1 / ( 1 + exp(-X %*% beta))
y = c(runif(n) < py)
df_r = function( b, index){
sigmoid = 1 / (1 + exp(-c(X[index,] %*% b)))
return( colSums((sigmoid - y[index]) * X[index,]) )
}
# Plot the trace of the negative log-likelihood
neg_l_r = function( b, index){
sigmoid = 1 / (1 + exp(-c(X[index,] %*% b)) +10^-6)
return( -sum(y[index] * log(sigmoid) + (1 - y[index]) * log(1 - sigmoid)) )
}
b0 = runif(p) * 2*sqrt(6/p) + (-sqrt(6/p))
beta_MLE = SGD( r=100, n=n, gradient=df_r, x0=b0); tail( trace, 2); beta
beta_MLE = stochastic_gradient_descent( r=100, n=n, gradient=df_r, x0=b0); tail( trace, 2)
# Compute the predicted values on the training data
py_hat = 1 / (1 + exp(-c(X %*% beta_MLE)))
dim(beta_MLE)
beta_MLE = stochastic_gradient_descent( r=100, n=n, gradient=df_r, x0=b0)[1000,]
# Compute the predicted values on the training data
py_hat = 1 / (1 + exp(-c(X %*% beta_MLE)))
y_hat = (py_hat > .5) # Using a threshold of .5
# False positive rate
FPR = sum(y_hat[y == 0] == 1) / sum(y == 0); FPR
# True postive rate (i.e., sensitivity)
TPR = sum(y_hat[y == 1] == 1) / sum(y == 1); TPR
# True negative rate (i.e., specificity)
TNR = sum(y_hat[y == 0] == 0) / sum(y == 0); TNR
# How do these values change for different thresholds?
grid = seq( 0, 1, by=.001)
FPR = rep( NA, length(grid))
TPR = rep( NA, length(grid))
for(k in 1:length(grid)){
y_hat = (py_hat > grid[k])
FPR[k] = sum(y_hat[y == 0] == 1) / sum(y == 0)
TPR[k] = sum(y_hat[y == 1] == 1) / sum(y == 1)
}
# Plot the ROC curve
plot( FPR, TPR, lwd=2, type="l")
lines( grid, grid, lty="dotted", lwd=2)
mini_batch_gradient_descent = function( r, n, gradient, eta=.01, x0, epsilon=10^-3, max_iter=10^4){
t = 2
trace = matrix( NA, ncol=length(x0), nrow=max_iter*n)
trace[1:2,] =	rbind( x0, x0 + 2*epsilon)
# Begin the gradient descent algorithm
while( sum((trace[t,] - trace[t-1,])^2)^.5 >= epsilon & t < max_iter ){
for(s in 0:(floor(n/r)-1)){
index = (s*r +1):(s*r +r)
# Update the parameters
trace[t+1,] = trace[t,] - eta * gradient( trace[t,], index)
#print(trace[t+1,])
t = t +1
}
}
cat("final learning rate = ", eta, "\n")
if(t == max_iter){
print("Warning: max iterations exceeded")
trace[max_iter,] = NA
}
return(trace[1:t,])
}
n = 2000
p = 1000
beta = runif( n=p, min=1, max=5) * (-1)^(runif(p) < .5)
X = cbind( 1, matrix( rnorm(n*(p-1)), ncol=p-1))
py = 1 / ( 1 + exp(-X %*% beta))
y = c(runif(n) < py)
b0 = runif(p) * 2*sqrt(6/p) + (-sqrt(6/p))
beta_MLE = mini_batch_gradient_descent( r=100, n=n, gradient=df_r, x0=b0)[1000,]
# ST 495 Homework 8
# Tyson King
# -----------------------------------------------------------------------------
# Question 1.
gradient_descent <- function( gradient, f, eta=.01, x0, epsilon=10^-6, max_iter=10^4){
t = 2
trace = matrix( NA, ncol=length(x0), nrow=max_iter)
trace[1:2,] =	rbind( x0, x0 + 2*epsilon)
neg_l = f(trace[t,])
# Begin the gradient descent algorithm
while( sum((trace[t,] - trace[t-1,])^2)^.5 >= epsilon & t < max_iter ){
# Update the parameters
trace[t+1,] = trace[t,] - eta * gradient(trace[t,])
#print(trace[t+1,])
t = t +1
# If negative log-likelihood increases, then reduce learning rate
neg_l_new = f(trace[t,])
#cat(neg_l_new, "  ", neg_l, "\n")
if(neg_l_new >= neg_l) eta = eta * .9
neg_l = neg_l_new
}
cat("final learning rate = ", eta, "\n")
if(t == max_iter){
print("Warning: max iterations exceeded")
trace[max_iter,] = NA
}
return(trace[1:t,])
}
set.seed(123)
n = 1000
p = 300
beta = runif( n=p, min=1, max=5) * (-1)^(runif(p) < .5)
X = cbind( 1, matrix( rnorm(n*(p-1)), ncol=p-1))
py = 1 / ( 1 + exp(-X %*% beta))
y = c(runif(n) < py)
df = function(b){
sigmoid = 1 / (1 + exp(-c(X %*% b)))
return( colSums((sigmoid - y) * X) )
}
neg_l = function(b){
sigmoid = 1 / (1 + exp(-c(X %*% b)) +10^-6) # 10^-6 for numerical stability
return( -sum(y * log(sigmoid) + (1 - y) * log(1 - sigmoid)) )
}
b0 = runif(p) * 2*sqrt(6/p) + (-sqrt(6/p))
beta_MLE = gradient_descent( gradient=df, f=neg_l, x0=b0)[1000,]
py_hat = 1 / (1 + exp(-c(X %*% beta_MLE)))
y_hat = (py_hat > .5) # Using a threshold of .5
# False positive rate
FPR = sum(y_hat[y == 0] == 1) / sum(y == 0); FPR
# True postive rate (i.e., sensitivity)
TPR = sum(y_hat[y == 1] == 1) / sum(y == 1); TPR
# True negative rate (i.e., specificity)
TNR = sum(y_hat[y == 0] == 0) / sum(y == 0); TNR
# How do these values change for different thresholds?
grid = seq( 0, 1, by=.001)
FPR = rep( NA, length(grid))
TPR = rep( NA, length(grid))
for(k in 1:length(grid)){
y_hat = (py_hat > grid[k])
FPR[k] = sum(y_hat[y == 0] == 1) / sum(y == 0)
TPR[k] = sum(y_hat[y == 1] == 1) / sum(y == 1)
}
# Plot the ROC curve
plot( FPR, TPR, lwd=2, type="l")
lines( grid, grid, lty="dotted", lwd=2)
## It seems like all of the y_hat values are exactly correct, not sure why
## that happened or if its supposed to happen that way but I can't fix it
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Question 2
stochastic_gradient_descent = function( r, n, gradient, eta=.01, x0, epsilon=10^-3, max_iter=10^4){
t = 2
trace = matrix( NA, ncol=length(x0), nrow=max_iter)
trace[1:2,] =	rbind( x0, x0 + 2*epsilon)
# Begin the gradient descent algorithm
while( sum((trace[t,] - trace[t-1,])^2)^.5 >= epsilon & t < max_iter ){
index = sample( 1:n, size=r, replace=F)
# Update the parameters
trace[t+1,] = trace[t,] - eta * gradient( trace[t,], index)
#print(trace[t+1,])
t = t +1
}
cat("final learning rate = ", eta, "\n")
if(t == max_iter){
print("Warning: max iterations exceeded")
trace[max_iter,] = NA
}
return(trace[1:t,])
}
n = 2000
p = 1000
beta = runif( n=p, min=1, max=5) * (-1)^(runif(p) < .5)
X = cbind( 1, matrix( rnorm(n*(p-1)), ncol=p-1))
py = 1 / ( 1 + exp(-X %*% beta))
y = c(runif(n) < py)
df_r = function( b, index){
sigmoid = 1 / (1 + exp(-c(X[index,] %*% b)))
return( colSums((sigmoid - y[index]) * X[index,]) )
}
# Plot the trace of the negative log-likelihood
neg_l_r = function( b, index){
sigmoid = 1 / (1 + exp(-c(X[index,] %*% b)) +10^-6)
return( -sum(y[index] * log(sigmoid) + (1 - y[index]) * log(1 - sigmoid)) )
}
b0 = runif(p) * 2*sqrt(6/p) + (-sqrt(6/p))
beta_MLE = stochastic_gradient_descent( r=100, n=n, gradient=df_r, x0=b0)[1000,]
# Compute the predicted values on the training data
py_hat = 1 / (1 + exp(-c(X %*% beta_MLE)))
y_hat = (py_hat > .5) # Using a threshold of .5
# False positive rate
FPR = sum(y_hat[y == 0] == 1) / sum(y == 0); FPR
# True postive rate (i.e., sensitivity)
TPR = sum(y_hat[y == 1] == 1) / sum(y == 1); TPR
# True negative rate (i.e., specificity)
TNR = sum(y_hat[y == 0] == 0) / sum(y == 0); TNR
# How do these values change for different thresholds?
grid = seq( 0, 1, by=.001)
FPR = rep( NA, length(grid))
TPR = rep( NA, length(grid))
for(k in 1:length(grid)){
y_hat = (py_hat > grid[k])
FPR[k] = sum(y_hat[y == 0] == 1) / sum(y == 0)
TPR[k] = sum(y_hat[y == 1] == 1) / sum(y == 1)
}
# Plot the ROC curve
plot( FPR, TPR, lwd=2, type="l")
lines( grid, grid, lty="dotted", lwd=2)
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Question 3
mini_batch_gradient_descent = function( r, n, gradient, eta=.01, x0, epsilon=10^-3, max_iter=10^4){
t = 2
trace = matrix( NA, ncol=length(x0), nrow=max_iter*n)
trace[1:2,] =	rbind( x0, x0 + 2*epsilon)
# Begin the gradient descent algorithm
while( sum((trace[t,] - trace[t-1,])^2)^.5 >= epsilon & t < max_iter ){
for(s in 0:(floor(n/r)-1)){
index = (s*r +1):(s*r +r)
# Update the parameters
trace[t+1,] = trace[t,] - eta * gradient( trace[t,], index)
#print(trace[t+1,])
t = t +1
}
}
cat("final learning rate = ", eta, "\n")
if(t == max_iter){
print("Warning: max iterations exceeded")
trace[max_iter,] = NA
}
return(trace[1:t,])
}
n = 2000
p = 1000
beta = runif( n=p, min=1, max=5) * (-1)^(runif(p) < .5)
X = cbind( 1, matrix( rnorm(n*(p-1)), ncol=p-1))
py = 1 / ( 1 + exp(-X %*% beta))
y = c(runif(n) < py)
b0 = runif(p) * 2*sqrt(6/p) + (-sqrt(6/p))
beta_MLE = mini_batch_gradient_descent( r=100, n=n, gradient=df_r, x0=b0)[1000,]
beta_MLE = mini_batch_gradient_descent( r=10, n=n, gradient=df_r, x0=b0)[1000,]
devtools::install_github("sportsdataverse/softballR")
scoreboard <- load_ncaa_scoreboard(2023, division = "D2") %>%
distinct()
knitr::opts_chunk$set(echo = FALSE)
#devtools::install_github("sportsdataverse/softballR")
library(softballR)
#install.packages("DT", repos = "http://cran.us.r-project.org")
library(DT)
#install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)
#install.packages("fmsb", repos = "http://cran.us.r-project.org")
library(fmsb)
get_current_rpi <- function(scoreboard){
team1_scoreboard <- scoreboard[c(9,1,4,5,8)] %>% `names<-`(c("date","team_name","runs","opponent_name","opponent_runs"))
team2_scoreboard <- scoreboard[c(9,5,8,1,4)] %>% `names<-`(c("date","team_name","runs","opponent_name","opponent_runs"))
scoreboard_upd <- rbind(team1_scoreboard, team2_scoreboard) %>%
mutate(win = case_when(runs > opponent_runs ~ 1,
runs < opponent_runs ~ 0,
runs == opponent_runs ~ 0.5))
win_perc <- scoreboard_upd %>%
group_by(team_name) %>%
summarise(games = n(),
win_perc = mean(win)) %>%
select(-games) %>%
drop_na()
scoreboard_upd_2 <- scoreboard_upd %>%
merge(win_perc, by.x = "opponent_name", by.y = "team_name") %>%
rename(opponent_win_perc = win_perc) %>%
merge(win_perc, by = "team_name")
opponent_win_perc <- scoreboard_upd_2 %>%
group_by(team_name) %>%
summarise(opponent_opponent_win_perc = mean(opponent_win_perc))
scoreboard_upd_3 <- scoreboard_upd_2 %>%
merge(opponent_win_perc, by.x = "opponent_name", by.y = "team_name")
rpi <- scoreboard_upd_3 %>%
group_by(team_name) %>%
summarise(rpi_coef = (.5 * mean(win_perc) + .25 * mean(opponent_win_perc) + .25 * mean(opponent_opponent_win_perc)),
record = paste(floor(sum(win)),floor(n() - sum(win)),ceiling(sum(win) %% 1), sep = "-")) %>%
ungroup() %>%
mutate(rpi_rank = rank(-rpi_coef))
return(rpi)
}
get_power_ratings <- function(scoreboard){
scoreboard_longer <- rbind(scoreboard[c(9,1,4,5,8)] %>% `names<-`(c("date", "team", "runs", "opponent", "opponent_runs")),
scoreboard[c(9,5,8,1,4)] %>% `names<-`(c("date", "team", "runs", "opponent", "opponent_runs")))
rpi <- get_current_rpi(scoreboard) %>%
select(team_name, rpi_rank)
sos <- scoreboard_longer %>%
merge(rpi, by.x = "opponent", by.y = "team_name") %>%
group_by(team) %>%
summarise(avg_opponent_rpi = mean(rpi_rank)) %>%
ungroup() %>%
mutate(rank = rank(avg_opponent_rpi)) %>%
select(team, rank)
runs_scored <- scoreboard_longer %>%
group_by(team) %>%
summarise(avg_runs_scored = mean(runs),
games = n()) %>%
select(-games) %>%
drop_na()
runs_allowed <- scoreboard_longer %>%
group_by(team) %>%
summarise(avg_runs_allowed = mean(opponent_runs),
games = n()) %>%
select(-games) %>%
drop_na()
best_offenses <- scoreboard_longer %>%
merge(runs_allowed, by.x = "opponent", by.y = "team") %>%
mutate(diff = runs - avg_runs_allowed) %>%
group_by(team) %>%
summarise(offensive_rating = mean(diff),
games = n()) %>%
ungroup() %>%
drop_na()
best_defenses <- scoreboard_longer %>%
merge(runs_scored, by.x = "opponent", by.y = "team") %>%
mutate(diff = avg_runs_scored - opponent_runs) %>%
group_by(team) %>%
summarise(defensive_rating = mean(diff),
games = n()) %>%
ungroup() %>%
drop_na()
standings <- scoreboard_longer %>%
group_by(team) %>%
summarise(wins = sum(runs > opponent_runs),
losses = sum(runs < opponent_runs),
ties = sum(runs == opponent_runs),
win_perc = wins / (wins + losses),
games = sum(wins, losses, ties)) %>%
drop_na() %>%
merge(best_offenses, by = "team") %>%
merge(best_defenses, by = "team") %>%
merge(sos, by = "team") %>%
select(team, wins, losses, ties, win_perc, offensive_rating, defensive_rating, rank)
load("~/Desktop/Projects/softball-projects/power_rating_winperc_model.RDA")
standings$overall_rating <- predict(model, standings) - coef(model)["rank"] * standings$rank
standings$power_rating <- (standings$overall_rating-min(standings$overall_rating)) /
(max(standings$overall_rating - min(standings$overall_rating)))
return(standings)
}
scoreboard <- load_ncaa_scoreboard(2023, division = "D2") %>%
distinct()
ratings <- get_power_ratings(scoreboard) %>%
filter(wins + losses + ties >= 20) %>%
select(-overall_rating) %>%
arrange(desc(power_rating))
load("~/Projects/softball-projects/power_rating_winperc_model.RDA")
var(c(31,29,25,35,15,31,22,27,25,19,30,18,21,40,38,28,17,22,41,32,35,19,29,18,31))
var(c(1590,1510,1490,1610,800,1720,1310,1427,1290,860,1620,710,1140,1980,1990,1420,900,1080,2010,1740,1750,890,1470,910,1740))
scoreboard <- load_ncaa_scoreboard(2023, division = "D2")
View(scoreboard)
box <- get_ncaa_player_box(2387864)
box$Hitting
View(box)
pbp <- get_ncaa_pbp(2387864)
View(pbp)
library(tidyverse)
library(softballR)
library(glue)
library(anytime)
library(gt)
library(gtExtras)
scoreboard <- load_ncaa_scoreboard(2023)
ratings <- get_power_ratings(scoreboard) %>%
select(team, power_rating)
library(softballR)
library(tidyverse)
setwd("~/Projects/softball-projects/ACC Softball Analytics/sim_season")
source("get_current_rpi.R")
source("get_power_ratings.R")
source("acc_season_sims.R")
scoreboard <- load_ncaa_scoreboard(2023)
ratings <- get_power_ratings(scoreboard) %>%
select(team, power_rating)
logos <- scoreboard %>%
distinct(home_team, home_team_logo)
acc_teams <- c("Clemson", "Duke", "Louisville", "Florida St.", "Virginia Tech", "Georgia Tech",
"North Carolina", "Notre Dame", "Virginia", "Pittsburgh", "Boston College", "NC State", "Syracuse")
team_ids <- get_ncaa_teams(2023) %>%
filter(team_name %in% acc_teams)
completed <- scoreboard %>%
filter(away_team %in% acc_teams & home_team %in% acc_teams)
completed <- rbind(completed[c(9,1,4,5,8)] %>% `names<-`(c("date", "team", "runs", "opponent", "opponent_runs")),
completed[c(9,5,8,1,4)] %>% `names<-`(c("date", "team", "runs", "opponent", "opponent_runs"))) %>%
mutate(win = runs > opponent_runs,
tie = runs == opponent_runs)
current_standings <- completed %>%
group_by(team) %>%
summarise(games = n(),
wins = sum(win),
ties = sum(tie),
losses = games - wins - ties,
record = paste0(wins,"-",losses,"-",ties))
remaining <- data.frame()
for(i in 1:nrow(team_ids)){
raw <- glue("https://stats.ncaa.org/teams/{team_ids$team_id[i]}") %>%
readLines()
teams <- acc_teams[which(acc_teams != team_ids$team_name[i])]
locs <- grep(paste(teams, collapse = "|"), raw)
date <- gsub('<[^>]+>', '', raw[locs - 2]) %>%
trimws() %>%
str_remove_all("\\(1\\)|\\(2\\)")
opponent <- gsub('.*alt="([^"]+)".*', '\\1', raw[locs])
games <- data.frame(team = team_ids$team_name[i], opponent, date) %>%
filter(anydate(date) > Sys.Date())
remaining <- rbind(remaining, games)
}
View(remaining)
remaining <- data.frame()
for(i in 1:nrow(team_ids)){
raw <- glue("https://stats.ncaa.org/teams/{team_ids$team_id[i]}") %>%
readLines()
teams <- acc_teams[which(acc_teams != team_ids$team_name[i])]
locs <- grep(paste(teams, collapse = "|"), raw)
date <- gsub('<[^>]+>', '', raw[locs - 2]) %>%
trimws() %>%
str_remove_all("\\(1\\)|\\(2\\)")
opponent <- gsub('.*alt="([^"]+)".*', '\\1', raw[locs])
games <- data.frame(team = team_ids$team_name[i], opponent, date) %>%
filter(anydate(date) >= Sys.Date())
remaining <- rbind(remaining, games)
}
View(completed)
library(softballR)
actual_rankings <- get_rankings("USA Today")
View(actual_rankings)
